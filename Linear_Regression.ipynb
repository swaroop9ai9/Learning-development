{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8dZmzVx47Fl",
        "colab_type": "code",
        "outputId": "660dc809-958d-4d08-dd52-ab509601fdf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "# We will learn the theory of Linear Regression and partly Model Validation & Hyperparameter tunning. \n",
        "# Finally we will predict adjusted stock price of Amazon for 30 days using previous adjusted price. (Univariate linear Regression)\n",
        " \n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import numpy as np"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r63YNu3QLYta",
        "colab_type": "code",
        "outputId": "59bc3e79-43bf-44e4-cea6-a9dd1e7f7b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "# y = a*x + b, is the simplest form of linear regression\n",
        "rng = np.random.RandomState(1)\n",
        "x = 10 * rng.rand(50)\n",
        "y = 2 * x - 5 + rng.randn(50)\n",
        "plt.scatter(x, y);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uogrZin3LiRO",
        "colab_type": "code",
        "outputId": "1cd95cce-65c0-4948-c950-b15893b97864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression(fit_intercept=True)\n",
        "\n",
        "model.fit(x[:, np.newaxis], y)\n",
        "\n",
        "xfit = np.linspace(0, 10, 1000)\n",
        "yfit = model.predict(xfit[:, np.newaxis])\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(xfit, yfit);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2rzXbdDLuyn",
        "colab_type": "code",
        "outputId": "f6c84d26-845a-41f4-e0c1-660657cb1614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Slope and Intercept are model's fit parameters\n",
        "print(\"Model slope:    \", model.coef_[0])\n",
        "print(\"Model intercept:\", model.intercept_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model slope:     2.0272088103606953\n",
            "Model intercept: -4.998577085553204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBRUmpp4L6SX",
        "colab_type": "code",
        "outputId": "f118203a-6a80-46da-e46d-be3da844bff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# We can scale linear Regression to many dimensions, In a multi dimensional space we can use \"Hyperplane\" to predict the target variable\n",
        "rng = np.random.RandomState(1)\n",
        "X = 10 * rng.rand(100, 3)\n",
        "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
        "# Y is being modelled from 3 X variables\n",
        "model.fit(X, y)\n",
        "print(model.intercept_)\n",
        "print(model.coef_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.50000000000001\n",
            "[ 1.5 -2.   1. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSwN3HMsMY_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Validation Using Holdout Technique\n",
        "\n",
        "''' We need to do model validation using holdout sets, we hold back some subset of the data from the training of the model, \n",
        "and then use this holdout set to check the model performance. This splitting can be done using the train_test_split utility in\n",
        " Scikit-Learn: (Example code)\n",
        "\n",
        "from sklearn.cross_validation import train_test_split\n",
        "# split the data with 50% in each set\n",
        "X1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n",
        "                                  train_size=0.5)\n",
        "\n",
        "# fit the model on one set of data\n",
        "model.fit(X1, y1)\n",
        "\n",
        "# evaluate the model on the second set of data\n",
        "y2_model = model.predict(X2) # Predicting the holdout data using the model\n",
        "accuracy_score(y2, y2_model) # Checking the accuracy score b/w predicted and actual lables of holdout data.\n",
        "'''\n",
        "# One disadvantage in holding data is it maybe reduce the amount of available data.This is not optimal, and can\n",
        "# cause problems ‚Äì especially if the initial set of training data is small.\n",
        "\n",
        "# Model Validation using Cross Validation technique.\n",
        "'''\n",
        "One way to address this is to use cross-validation; that is, to do a sequence of fits where each subset of the data is used both \n",
        "as a training set and as a validation set. Visually, it might look something like this:\n",
        "\n",
        "trail 1 : 50% Training, 50% Validation\n",
        "trail 2 : 50% Validation, 50% Training\n",
        "\n",
        "y2_model = model.fit(X1, y1).predict(X2) # We are using model to fit both parts of data and validate it both ways.\n",
        "y1_model = model.fit(X2, y2).predict(X1)\n",
        "accuracy_score(y1, y1_model), accuracy_score(y2, y2_model) # We can combine these accuracies to observe global performance.\n",
        "# We can use 2fold (like above), or 5fold cross validation technique.\n",
        "\n",
        "from sklearn.cross_validation import cross_val_score\n",
        "cross_val_score(model, X, y, cv=5)\n",
        "\n",
        "#Repeating the validation across different subsets of the data gives us an even better idea of the performance of the algorithm.\n",
        "\n",
        "'''\n",
        "# Cross Validation using leave-one-out Method\n",
        "'''\n",
        "scikit-Learn implements a number of useful cross-validation schemes that are useful in particular situations; these are implemented via \n",
        "iterators in the cross_validation module. For example, we might wish to go to the extreme case in which our number of folds is equal to \n",
        "the number of data points: that is, we train on all points but one in each trial. This type of cross-validation is known as \n",
        "leave-one-out cross validation.\n",
        "\n",
        "from sklearn.cross_validation import LeaveOneOut\n",
        "scores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXdjgHahvv8E",
        "colab_type": "code",
        "outputId": "db6cc6d5-40cc-4f06-9d0f-982304b60eb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Model Selection \n",
        "'''\n",
        "Sometimes using a simple model yields better results than a complex model. Reducing unimportant parameters also helps very much it creating an efficient model\n",
        "'''\n",
        "# Bias-Varience Tradeoff\n",
        "'''\n",
        "  High Bias - Underfits the model (Performance on Training is similar to Test data)\n",
        "  High Varience - Overfits the model (Captures a lot of noise), (Performace of traning is far better than test data)\n",
        "'''\n",
        "# The score here is the  ùëÖ2  score, or coefficient of determination, which measures how well a model performs relative to a simple mean\n",
        "# of the target values.\n",
        "''' \n",
        "  sqr(R) = 1, perfect model\n",
        "  sqr(R) = 0, Means the model does no better than simply taking the mean of the data.\n",
        "A best model usually will perform optimally when the complexity of the model is medium and the accuracy is at its peak, but is the model \n",
        "has too high bias then it will underfit and be very generalized'''\n",
        "\n",
        "# Validation Curve for deciding on the model\n",
        "'''\n",
        "from sklearn.learning_curve import validation_curve\n",
        "degree = np.arange(0, 21)\n",
        "train_score, val_score = validation_curve(PolynomialRegression(), X, y,\n",
        "                                          'polynomialfeatures__degree', degree, cv=7)\n",
        "\n",
        "plt.plot(degree, np.median(train_score, 1), color='blue', label='training score')\n",
        "plt.plot(degree, np.median(val_score, 1), color='red', label='validation score')\n",
        "plt.legend(loc='best')\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel('degree')\n",
        "plt.ylabel('score');  '''\n",
        "\n",
        "# Learning Curve General Behavior\n",
        "# * A model of a given complexity will overfit a small dataset: this means the training score will be relatively high, \n",
        "#       while the validation score will be relatively low.\n",
        "# * A model of a given complexity will underfit a large dataset: this means that the training score\n",
        "#       will decrease, but the validation score will increase.\n",
        "# * A model will never, except by chance, give a better score to the validation set than the training set: \n",
        "#        this means the curves should keep getting closer together but never cross.\n",
        "\n",
        "''' This is a valuable diagnostic, because it gives us a visual depiction of how our model responds to increasing training data. \n",
        "In particular, when your learning curve has already converged (i.e., when the training and validation curves are already close \n",
        "to each other) adding more training data will not significantly improve the fit! This situation is seen in the left panel, with the \n",
        "learning curve for the degree-2 model.'''\n",
        "\n",
        "'''\n",
        "The only way to increase the converged score is to use a different (usually more complicated) model. We see this in the right panel: \n",
        "by moving to a much more complicated model, we increase the score of convergence (indicated by the dashed line), but at the expense \n",
        "of higher model variance (indicated by the difference between the training and validation scores). If we were to add even more data \n",
        "points, the learning curve for the more complicated model would eventually converge.\n",
        "'''\n",
        "\n",
        "# Validation in practice\n",
        "'''\n",
        "In practice, models generally have more than one knob to turn, and thus plots of validation and learning curves change from lines to\n",
        " multi-dimensional surfaces. In these cases, such visualizations are difficult and we would rather simply find the particular model \n",
        " that maximizes the validation score.\n",
        "\n",
        "Scikit-Learn provides automated tools to do this in the grid search module. \n",
        "\n",
        "from sklearn.grid_search import GridSearchCV\n",
        "\n",
        "param_grid = {'polynomialfeatures__degree': np.arange(21),\n",
        "              'linearregression__fit_intercept': [True, False],\n",
        "              'linearregression__normalize': [True, False]}\n",
        "\n",
        "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)\n",
        "grid.fit(X, y);\n",
        "grid.best_params_'''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nIn practice, models generally have more than one knob to turn, and thus plots of validation and learning curves change from lines to\\n multi-dimensional surfaces. In these cases, such visualizations are difficult and we would rather simply find the particular model \\n that maximizes the validation score.\\n\\nScikit-Learn provides automated tools to do this in the grid search module. \\n\\nfrom sklearn.grid_search import GridSearchCV\\n\\nparam_grid = {'polynomialfeatures__degree': np.arange(21),\\n              'linearregression__fit_intercept': [True, False],\\n              'linearregression__normalize': [True, False]}\\n\\ngrid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)\\ngrid.fit(X, y);\\ngrid.best_params_\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjvUxIov2A-_",
        "colab_type": "code",
        "outputId": "85b2a918-dd85-413d-ef1a-39a1455f0d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Feature Engineering\n",
        "'''\n",
        "Real World data is very messy, its not very welcoming at first sight, so we use feature engineering techiniques to make our model efficient and efficient\n",
        "We usually build our feature matrix, lets understand feature engineering for different aspect of data.'''\n",
        "\n",
        "#  1) Categorical Data\n",
        "# Consider data of house pricing and we have neighbourhood data which is categorical\n",
        "# Inorder to effectively represent the categorical data mathematically, we can use One-hot Encoding, which create additional columns\n",
        "# It indicate the presence or absence of a category by encoding it with 1 or 0\n",
        "'''\n",
        "  data = [\n",
        "    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n",
        "    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},\n",
        "    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},\n",
        "    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}\n",
        "] '''\n",
        "# When data comes as dictionary datatype, we can use DictVectorizer to apply one-hot encoding\n",
        "'''from sklearn.feature_extraction import DictVectorizer\n",
        "vec = DictVectorizer(sparse=False, dtype=int)\n",
        "vec.fit_transform(data) '''\n",
        "\n",
        "''' O/P :\n",
        "array([[     0,      1,      0, 850000,      4],\n",
        "       [     1,      0,      0, 700000,      3],\n",
        "       [     0,      0,      1, 650000,      3],\n",
        "       [     1,      0,      0, 600000,      2]], dtype=int64)  \n",
        "  \n",
        "Notice that the 'neighborhood' column has been expanded into three separate columns, representing the three neighborhood labels,\n",
        "and that each row has a 1 in the column associated with its neighborhood. With these categorical features thus encoded,\n",
        "you can proceed as normal with fitting a Scikit-Learn model.'''\n",
        "\n",
        "# To get the names of actual features:\n",
        "# vec.get_feature_names()\n",
        "# Although it looks exhausting, we can use sparse option as majority of it has zeroes\n",
        "'''\n",
        "vec = DictVectorizer(sparse=True, dtype=int)\n",
        "vec.fit_transform(data) # We can use 'One hot Encoder' or 'Feature Hasher' to handle feature data '''\n",
        "\n",
        " \n",
        "# 2) Textual Data\n",
        "# The simplest Idea to convert text data to numerical data is to bucket each word into its word counts\n",
        "'''\n",
        "sample = ['problem of evil',\n",
        "          'evil queen',\n",
        "          'horizon problem']\n",
        "# We can use CountVectorizer to convert the above sample text in to buckets\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer()\n",
        "X = vec.fit_transform(sample)\n",
        "import pandas as pd\n",
        "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
        "\n",
        "Output:   \n",
        "    evil\thorizon\tof\tproblem\tqueen\n",
        "0\t    1\t    0\t     1\t  1\t      0\n",
        "1\t    1\t    0\t     0\t  0\t      1\n",
        "2\t    0\t    1\t     0\t  1\t      0\n",
        "\n",
        "There are some issues with this approach, however: the raw word counts lead to features which put too much weight on words that \n",
        "appear very frequently, and this can be sub-optimal in some classification algorithms. One approach to fix this is known as \n",
        "term frequency-inverse document frequency (TF‚ÄìIDF) which weights the word counts by a measure of how often they appear in the documents. \n",
        "'''\n",
        "# Term-Frequency-Inverse-Document-Frequency (TF-IDF)\n",
        "'''\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer()\n",
        "X = vec.fit_transform(sample)\n",
        "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
        "'''\n",
        "\n",
        "# Image Features\n",
        "# Usually the best ideal way is to perform computations on Pixel Values, But depending on the application this approach might not be optimal\n",
        "\n",
        "# Derived Features\n",
        "# Usually converting polynomial features by transforming the input\n",
        "'''%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([4, 2, 1, 3, 7])\n",
        "plt.scatter(x, y);'''\n",
        "\n",
        "# Fitting it to linear regression \n",
        "'''\n",
        "from sklearn.linear_model import LinearRegression\n",
        "X = x[:, np.newaxis]\n",
        "model = LinearRegression().fit(X, y)\n",
        "yfit = model.predict(X)\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, yfit);  '''\n",
        "\n",
        "# Insearch for a more sophisticated model we can add extra column with polynomial features of input data\n",
        "'''\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
        "X2 = poly.fit_transform(X)\n",
        "print(X2)\n",
        "\n",
        "model = LinearRegression().fit(X2, y)\n",
        "yfit = model.predict(X2)\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, yfit);\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom sklearn.preprocessing import PolynomialFeatures\\npoly = PolynomialFeatures(degree=3, include_bias=False)\\nX2 = poly.fit_transform(X)\\nprint(X2)\\n\\nmodel = LinearRegression().fit(X2, y)\\nyfit = model.predict(X2)\\nplt.scatter(x, y)\\nplt.plot(x, yfit);\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qlBDCyA8GCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imputting Missing data using Feature Engineering\n",
        "from numpy import nan\n",
        "import numpy as np\n",
        "X = np.array([[ nan, 0,   3  ],\n",
        "              [ 3,   7,   9  ],\n",
        "              [ 3,   5,   2  ],\n",
        "              [ 4,   nan, 6  ],\n",
        "              [ 8,   8,   1  ]])\n",
        "y = np.array([14, 16, -1,  8, -5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhInWsv98OIU",
        "colab_type": "code",
        "outputId": "f9adc911-ca5a-47d9-b782-44255260197a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We can replace these missing values by simply replacing it with mean of the column or use more sophisticated approach of using domain \n",
        "# knowledge to replace the missing data\n",
        "# Inorder to impute mean, median, mode we can use 'Imputer' class\n",
        "'''\n",
        "from sklearn.preprocessing import Imputer\n",
        "imp = Imputer(strategy='mean')\n",
        "X2 = imp.fit_transform(X)\n",
        "X2'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom sklearn.preprocessing import Imputer\\nimp = Imputer(strategy='mean')\\nX2 = imp.fit_transform(X)\\nX2\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95vnpbbG860H",
        "colab_type": "code",
        "outputId": "3b3f7e44-9e8a-4bef-f2a4-56c1172e54c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# It seems very tedious task to replace the missing values or perform feature engineering by hand, so in order to streamline such process\n",
        "# we can use a processing pipeline for such tasks \n",
        "'''\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(Imputer(strategy='mean'),\n",
        "                      PolynomialFeatures(degree=2),\n",
        "                      LinearRegression())\n",
        "model.fit(X, y)  # X with missing values, from above\n",
        "print(y)\n",
        "print(model.predict(X))'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom sklearn.pipeline import make_pipeline\\n\\nmodel = make_pipeline(Imputer(strategy='mean'),\\n                      PolynomialFeatures(degree=2),\\n                      LinearRegression())\\nmodel.fit(X, y)  # X with missing values, from above\\nprint(y)\\nprint(model.predict(X))\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DchAeMwP-BOF",
        "colab_type": "code",
        "outputId": "815068bf-e4b5-4921-8c50-bfaed24f53a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Gaussian Basic Functions\n",
        "# one useful pattern is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian bases. ( Adding the bases of normal distributions and combining them)\n",
        "'''from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class GaussianFeatures(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Uniformly spaced Gaussian features for one-dimensional input\"\"\"\n",
        "    \n",
        "    def __init__(self, N, width_factor=2.0):\n",
        "        self.N = N\n",
        "        self.width_factor = width_factor\n",
        "    \n",
        "    @staticmethod\n",
        "    def _gauss_basis(x, y, width, axis=None):\n",
        "        arg = (x - y) / width\n",
        "        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        # create N centers spread along the data range\n",
        "        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n",
        "        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n",
        "        return self\n",
        "        \n",
        "    def transform(self, X):\n",
        "        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\n",
        "                                 self.width_, axis=1)\n",
        "    \n",
        "gauss_model = make_pipeline(GaussianFeatures(20),\n",
        "                            LinearRegression())\n",
        "gauss_model.fit(x[:, np.newaxis], y)\n",
        "yfit = gauss_model.predict(xfit[:, np.newaxis])\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(xfit, yfit)\n",
        "plt.xlim(0, 10);'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from sklearn.base import BaseEstimator, TransformerMixin\\n\\nclass GaussianFeatures(BaseEstimator, TransformerMixin):\\n    \"\"\"Uniformly spaced Gaussian features for one-dimensional input\"\"\"\\n    \\n    def __init__(self, N, width_factor=2.0):\\n        self.N = N\\n        self.width_factor = width_factor\\n    \\n    @staticmethod\\n    def _gauss_basis(x, y, width, axis=None):\\n        arg = (x - y) / width\\n        return np.exp(-0.5 * np.sum(arg ** 2, axis))\\n        \\n    def fit(self, X, y=None):\\n        # create N centers spread along the data range\\n        self.centers_ = np.linspace(X.min(), X.max(), self.N)\\n        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\\n        return self\\n        \\n    def transform(self, X):\\n        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\\n                                 self.width_, axis=1)\\n    \\ngauss_model = make_pipeline(GaussianFeatures(20),\\n                            LinearRegression())\\ngauss_model.fit(x[:, np.newaxis], y)\\nyfit = gauss_model.predict(xfit[:, np.newaxis])\\n\\nplt.scatter(x, y)\\nplt.plot(xfit, yfit)\\nplt.xlim(0, 10);'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r44c-lKj_LPA",
        "colab_type": "code",
        "outputId": "1f8c4bc9-9bc3-44a1-b12f-69c87c61a567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Regularization \n",
        "# Sometimes basic functions can also overfit the data, ex: too many gaussian bases might also overfit the data\n",
        "'''model = make_pipeline(GaussianFeatures(30),\n",
        "                      LinearRegression())\n",
        "model.fit(x[:, np.newaxis], y)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
        "\n",
        "plt.xlim(0, 10)\n",
        "plt.ylim(-1.5, 1.5);'''\n",
        "\n",
        "'''\n",
        "# Basis_polt gives the deviation of curve from reality or amplitude of basis function at each location\n",
        "def basis_plot(model, title=None):\n",
        "    fig, ax = plt.subplots(2, sharex=True)\n",
        "    model.fit(x[:, np.newaxis], y)\n",
        "    ax[0].scatter(x, y)\n",
        "    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
        "    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))\n",
        "    \n",
        "    if title:\n",
        "        ax[0].set_title(title)\n",
        "\n",
        "    ax[1].plot(model.steps[0][1].centers_,\n",
        "               model.steps[1][1].coef_)\n",
        "    ax[1].set(xlabel='basis location',\n",
        "              ylabel='coefficient',\n",
        "              xlim=(0, 10))\n",
        "    \n",
        "model = make_pipeline(GaussianFeatures(30), LinearRegression())\n",
        "basis_plot(model) '''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Basis_polt gives the deviation of curve from reality or amplitude of basis function at each location\\ndef basis_plot(model, title=None):\\n    fig, ax = plt.subplots(2, sharex=True)\\n    model.fit(x[:, np.newaxis], y)\\n    ax[0].scatter(x, y)\\n    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\\n    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))\\n    \\n    if title:\\n        ax[0].set_title(title)\\n\\n    ax[1].plot(model.steps[0][1].centers_,\\n               model.steps[1][1].coef_)\\n    ax[1].set(xlabel='basis location',\\n              ylabel='coefficient',\\n              xlim=(0, 10))\\n    \\nmodel = make_pipeline(GaussianFeatures(30), LinearRegression())\\nbasis_plot(model) \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv2AnmSkVTs-",
        "colab_type": "code",
        "outputId": "b0e154bd-9b6f-4f58-e58f-cecf7c37f6ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Ridge regression ( ùêø2  Regularization)\n",
        "'''\n",
        "Perhaps the most common form of regularization is known as ridge regression or  ùêø2  regularization, sometimes also called Tikhonov \n",
        "regularization. This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the \n",
        "model fit would be\n",
        "                      ùëÉ=ùõº ‚àë sqr(ùúÉ) (from n=1 to N)\n",
        "\n",
        "where ùõº is a free parameter that controls the strength of the penalty. This type of penalized model is built into Scikit-Learn with the Ridge estimator:\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "model = make_pipeline(GaussianFeatures(30), Ridge(alpha=0.1))\n",
        "basis_plot(model, title='Ridge Regression') '''\n",
        "\n",
        "# ùõº Parameter\n",
        "''' \n",
        "The  ùõº  parameter is essentially a knob controlling the complexity of the resulting model.\n",
        "In the limit  ùõº‚Üí0 , we recover the standard linear regression result; in the limit  ùõº‚Üí‚àû , all model responses will be suppressed.\n",
        "One advantage of ridge regression in particular is that it can be computed very efficiently‚Äîat hardly more computational cost \n",
        "than the original linear regression model.  '''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \\nThe  ùõº  parameter is essentially a knob controlling the complexity of the resulting model.\\nIn the limit  ùõº‚Üí0 , we recover the standard linear regression result; in the limit  ùõº‚Üí‚àû , all model responses will be suppressed.\\nOne advantage of ridge regression in particular is that it can be computed very efficiently‚Äîat hardly more computational cost \\nthan the original linear regression model.  '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4Ez6vX2V-3z",
        "colab_type": "code",
        "outputId": "e619fb7f-5df1-431b-b56b-d1383525e940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Lasso regression ( ùêø1  regularization)\n",
        "'''\n",
        "Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:\n",
        "        \n",
        "                                  ùëÉ=ùõº ‚àë |ùúÉùëõ|  (from n=1 to N)\n",
        "Though this is conceptually very similar to ridge regression, the results can differ surprisingly: \n",
        "for example, due to geometric reasons lasso regression tends to favor sparse models where possible: that is, it preferentially sets model coefficients to exactly zero.\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "model = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.001))\n",
        "basis_plot(model, title='Lasso Regression')\n",
        "\n",
        "With the lasso regression penalty, the majority of the coefficients are exactly zero, with the functional behavior being modeled \n",
        "by a small subset of the available basis functions. As with ridge regularization, the  ùõº  parameter tunes the strength of the \n",
        "penalty, and should be determined via, for example, cross-validation.'''\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nAnother very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:\\n        \\n                                  ùëÉ=ùõº ‚àë |ùúÉùëõ|  (from n=1 to N)\\nThough this is conceptually very similar to ridge regression, the results can differ surprisingly: \\nfor example, due to geometric reasons lasso regression tends to favor sparse models where possible: that is, it preferentially sets model coefficients to exactly zero.\\n\\nfrom sklearn.linear_model import Lasso\\nmodel = make_pipeline(GaussianFeatures(30), Lasso(alpha=0.001))\\nbasis_plot(model, title='Lasso Regression')\\n\\nWith the lasso regression penalty, the majority of the coefficients are exactly zero, with the functional behavior being modeled \\nby a small subset of the available basis functions. As with ridge regularization, the  ùõº  parameter tunes the strength of the \\npenalty, and should be determined via, for example, cross-validation.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1wydtCSYm5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict the prices for stock 30 days by the current Adjusted Close price.\n",
        "#Install the dependencies\n",
        "#!pip3 install quandl\n",
        "import quandl\n",
        "import numpy as np \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyAmwproZI7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "75c3f56b-f13b-42a5-99df-98a889ad19dc"
      },
      "source": [
        "# Get the stock data\n",
        "df = quandl.get(\"WIKI/AMZN\")\n",
        "# Take a look at the data\n",
        "print(df.head())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             Open   High    Low  ...  Adj. Low  Adj. Close  Adj. Volume\n",
            "Date                             ...                                   \n",
            "1997-05-16  22.38  23.75  20.50  ...  1.708333    1.729167   14700000.0\n",
            "1997-05-19  20.50  21.25  19.50  ...  1.625000    1.708333    6106800.0\n",
            "1997-05-20  20.75  21.00  19.63  ...  1.635833    1.635833    5467200.0\n",
            "1997-05-21  19.25  19.75  16.50  ...  1.375000    1.427500   18853200.0\n",
            "1997-05-22  17.25  17.38  15.75  ...  1.312500    1.395833   11776800.0\n",
            "\n",
            "[5 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIoSapF3ZfHF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "7e38c30a-696f-40bb-c076-d2cc39ddc75c"
      },
      "source": [
        "# Lets use Adjusted Close column to predict the Adjusted close for the next 30 days\n",
        "# Get the Adjusted Close Price \n",
        "df = df[['Adj. Close']] \n",
        "# Take a look at the new data \n",
        "print(df.head(10))\n",
        "print(df.tail(10))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Adj. Close\n",
            "Date                  \n",
            "1997-05-16    1.729167\n",
            "1997-05-19    1.708333\n",
            "1997-05-20    1.635833\n",
            "1997-05-21    1.427500\n",
            "1997-05-22    1.395833\n",
            "1997-05-23    1.500000\n",
            "1997-05-27    1.583333\n",
            "1997-05-28    1.531667\n",
            "1997-05-29    1.505000\n",
            "1997-05-30    1.500000\n",
            "            Adj. Close\n",
            "Date                  \n",
            "2018-03-14     1591.00\n",
            "2018-03-15     1582.32\n",
            "2018-03-16     1571.68\n",
            "2018-03-19     1544.93\n",
            "2018-03-20     1586.51\n",
            "2018-03-21     1581.86\n",
            "2018-03-22     1544.10\n",
            "2018-03-23     1495.56\n",
            "2018-03-26     1555.86\n",
            "2018-03-27     1497.05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfW_fvRvZz4o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "296c5747-344b-423e-b520-32098073d3fd"
      },
      "source": [
        "# A variable for predicting 'n' days out into the future\n",
        "forecast_out = 30 #'n=30' days\n",
        "#Create another column (the target ) shifted 'n' units up\n",
        "df['Prediction'] = df[['Adj. Close']].shift(-forecast_out) # used to shift rows or columns data by certain number.\n",
        "#print the new data set\n",
        "print(df.tail(35)) # We can see that the prediction column has shifted by 30 spaces up and now has the Adj. Close values"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            Adj. Close  Prediction\n",
            "Date                              \n",
            "2018-02-06     1442.84     1581.86\n",
            "2018-02-07     1416.78     1544.10\n",
            "2018-02-08     1350.47     1495.56\n",
            "2018-02-09     1338.99     1555.86\n",
            "2018-02-12     1386.23     1497.05\n",
            "2018-02-13     1414.51         NaN\n",
            "2018-02-14     1451.05         NaN\n",
            "2018-02-15     1461.76         NaN\n",
            "2018-02-16     1448.69         NaN\n",
            "2018-02-20     1468.35         NaN\n",
            "2018-02-21     1482.92         NaN\n",
            "2018-02-22     1484.76         NaN\n",
            "2018-02-23     1500.00         NaN\n",
            "2018-02-26     1521.95         NaN\n",
            "2018-02-27     1511.98         NaN\n",
            "2018-02-28     1512.45         NaN\n",
            "2018-03-01     1493.45         NaN\n",
            "2018-03-02     1500.25         NaN\n",
            "2018-03-05     1523.61         NaN\n",
            "2018-03-06     1537.64         NaN\n",
            "2018-03-07     1545.00         NaN\n",
            "2018-03-08     1551.86         NaN\n",
            "2018-03-09     1578.89         NaN\n",
            "2018-03-12     1598.39         NaN\n",
            "2018-03-13     1588.18         NaN\n",
            "2018-03-14     1591.00         NaN\n",
            "2018-03-15     1582.32         NaN\n",
            "2018-03-16     1571.68         NaN\n",
            "2018-03-19     1544.93         NaN\n",
            "2018-03-20     1586.51         NaN\n",
            "2018-03-21     1581.86         NaN\n",
            "2018-03-22     1544.10         NaN\n",
            "2018-03-23     1495.56         NaN\n",
            "2018-03-26     1555.86         NaN\n",
            "2018-03-27     1497.05         NaN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKc2jS9uaRka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a9355801-0bc6-4c3b-e919-1962d1001346"
      },
      "source": [
        "### Create the independent data set (X)  #######\n",
        "# Convert the dataframe to a numpy array\n",
        "X = np.array(df.drop(['Prediction'],1))\n",
        "#Remove the last '30' rows\n",
        "X = X[:-forecast_out]\n",
        "print(X) # We removed these 30 days to check if we are able to predict corretly for those 30 days"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   1.72916667]\n",
            " [   1.70833333]\n",
            " [   1.63583333]\n",
            " ...\n",
            " [1350.47      ]\n",
            " [1338.99      ]\n",
            " [1386.23      ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XLJ4nKebDsG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bf835f9f-5706-40d0-d719-1f19f382144d"
      },
      "source": [
        "### Create the dependent data set (y)  #####\n",
        "# Convert the dataframe to a numpy array \n",
        "y = np.array(df['Prediction'])\n",
        "# Get all of the y values except the last '30' rows\n",
        "y = y[:-forecast_out] # In order to train the data, we have the target variable ready\n",
        "print(y)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.54166667e+00 1.51583333e+00 1.58833333e+00 ... 1.49556000e+03\n",
            " 1.55586000e+03 1.49705000e+03]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou6-jp8sdPCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into 80% training and 20% testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4FftJq-d9bL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca3ee023-d4f4-4c9f-e6df-352ed3d085d5"
      },
      "source": [
        "# Create and train the Linear Regression  Model\n",
        "lr = LinearRegression()\n",
        "# Train the model\n",
        "lr.fit(x_train, y_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSVb-cpVeDcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe6acf9b-9545-4e6f-808b-1cd48cec38c3"
      },
      "source": [
        "# Testing Model: Score returns the coefficient of determination R^2 of the prediction. \n",
        "# The best possible score is 1.0\n",
        "lr_confidence = lr.score(x_test, y_test)\n",
        "print(\"lr confidence: \", lr_confidence)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr confidence:  0.9871644663794275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-aIfXGNeH6A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "f8d424f4-6fed-49ed-c33d-6e8b9ad5f4d8"
      },
      "source": [
        "# Set x_forecast equal to the last 30 rows of the original data set from Adj. Close column\n",
        "x_forecast = np.array(df.drop(['Prediction'],1))[-forecast_out:]\n",
        "print(x_forecast)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1414.51]\n",
            " [1451.05]\n",
            " [1461.76]\n",
            " [1448.69]\n",
            " [1468.35]\n",
            " [1482.92]\n",
            " [1484.76]\n",
            " [1500.  ]\n",
            " [1521.95]\n",
            " [1511.98]\n",
            " [1512.45]\n",
            " [1493.45]\n",
            " [1500.25]\n",
            " [1523.61]\n",
            " [1537.64]\n",
            " [1545.  ]\n",
            " [1551.86]\n",
            " [1578.89]\n",
            " [1598.39]\n",
            " [1588.18]\n",
            " [1591.  ]\n",
            " [1582.32]\n",
            " [1571.68]\n",
            " [1544.93]\n",
            " [1586.51]\n",
            " [1581.86]\n",
            " [1544.1 ]\n",
            " [1495.56]\n",
            " [1555.86]\n",
            " [1497.05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZWOpMr8eO8N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "1495fab2-677c-466e-d205-acbe448a7930"
      },
      "source": [
        "# Print linear regression model predictions for the next '30' days\n",
        "lr_prediction = lr.predict(x_forecast)\n",
        "print(lr_prediction)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1499.59778631 1538.42227341 1549.80186446 1535.9147258  1556.80387239\n",
            " 1572.2847913  1574.23982842 1590.43263585 1613.75495363 1603.16162751\n",
            " 1603.66101199 1583.47312872 1590.69826589 1615.51873712 1630.42589513\n",
            " 1638.2460436  1645.53493198 1674.25485224 1694.9739956  1684.12566464\n",
            " 1687.12197153 1677.89929643 1666.5940818  1638.17166719 1682.35125596\n",
            " 1677.41053716 1637.28977545 1585.71504628 1649.78501267 1587.29820134]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVlX14nbeXBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# So, we are able to predict the adjusted cost for the stock of amazon for 30 days using previous 30 days as input."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtJij1Mfevfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}